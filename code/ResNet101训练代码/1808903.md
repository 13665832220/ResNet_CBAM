### 解压预训练参数


```python
# 解压预训练参数
!cd data/data88783 && unzip -qo ResNet101_pretrained.zip
```

### 解压水果叶片数据集


```python
# 解压水果叶片数据集

# 34分类
# !unzip data/data79818/leaf_train.zip
# !unzip data/data79818/leaf_test.zip

# 8分类
# !unzip data/data86514/leaf_train.zip
# !unzip data/data86514/leaf_test.zip

# 2分类
# !unzip data/data79796/leaf_train.zip
# !unzip data/data79796/leaf_test.zip

# 2分类_new
# !unzip data/data87406/leaf_train.zip
# !unzip data/data87406/leaf_test.zip

# 11分类
# !unzip data/data87385/leaf_train.zip
# !unzip data/data87385/leaf_test.zip

# 6分类
# !unzip data/data89551/leaf_train.zip
# !unzip data/data89551/leaf_test.zip

# !unzip data/data89987/leaf_train.zip
```


```python
# 引入os库,方便路径操作
import os
# 引入文件操作模块
import shutil
# 引入百度paddle模块
import paddle as paddle
# 引入百度飞桨的fluid模块,方便
import paddle.fluid as fluid
# 方便设置参数
from paddle.fluid.param_attr import ParamAttr
# 引入自行封装的reader文件
import reader
# 引入numpy库,方便计算和保存数据
import numpy as np
# 引入pandas库,方便使用
import pandas as pd
# 引入随机数
import random
# 引入日志库,方便记录操作的结果
import logging 
import codecs  
from PIL import Image  
# 引入画图的包
import matplotlib.pyplot as plt
```

### 数据采集与预处理

应用留出法，随机将90%的样本设置为训练集，10%的样本设置为测试集。




```python
# #按比例随机切割数据集
# train_ratio=0.9

# #tips: 小伙伴们可以在这里修改训练集和测试集的比例,比如0.5,0.7,0.9,比例越高效果相对来说较好

# train=open('train_split_list.txt','w')
# val=open('val_split_list.txt','w')
# # with open('data/data71961/train_list.txt','r') as f:
# with open('data/data87406/train_list.txt','r') as f:
#     #with open('data_sets/cat_12/train_split_list.txt','w+') as train:
#     lines=f.readlines()
#     for line in lines:
#         if random.uniform(0, 1) <= train_ratio: 
#             train.write(line) 
#         else: 
#             val.write(line)

# train.close()
# val.close()
                
```

### 训练配置
- 训练轮数
- 每批次训练图片数量
- 是否使用GPU训练
- 学习率调整
- 训练图片尺寸

如果不熟悉，请不要随便更改训练图片的尺寸，有可能需要调整模型的结构


```python
# -*- coding: UTF-8 -*-
"""
训练常用视觉基础网络，用于分类任务
需要将训练图片，类别文件 label_list.txt 放置在同一个文件夹下
程序会先读取 train.txt 文件获取类别数和图片数量
"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import os
import numpy as np
import time
import math
import paddle
import paddle.fluid as fluid
import codecs
import logging

from paddle.fluid.initializer import MSRA
from paddle.fluid.initializer import Uniform
from paddle.fluid.param_attr import ParamAttr
from PIL import Image
from PIL import ImageEnhance

train_parameters = {
    "input_size": [3, 224, 224],
    "class_dim": 2,  # 分类数，会在初始化自定义 reader 的时候获得
    "image_count": -1,  # 训练图片数量，会在初始化自定义 reader 的时候获得
    "label_dict": {},
    "data_dir": "data/data87406",  # 训练数据存储地址
    "train_file_list": "train_split_list.txt",
    "label_file": "label_list.txt",
    "save_freeze_dir": "./freeze-model",
    "save_persistable_dir": "./persistable-params",
    "continue_train": True,        # 是否接着上一次保存的参数接着训练，优先级高于预训练模型
    "pretrained": True,            # 是否使用预训练的模型
    "pretrained_dir": "data/data88783/ResNet101_pretrained", 
    "mode": "train",
    "num_epochs": 30,
    "train_batch_size": 32,
    "mean_rgb": [127.5, 127.5, 127.5],  # 常用图片的三通道均值，通常来说需要先对训练数据做统计，此处仅取中间值
    "use_gpu": True,
    "last_acc": 0.6,
    "image_enhance_strategy": {  # 图像增强相关策略
        "need_distort": True,  # 是否启用图像颜色增强
        "need_rotate": True,   # 是否需要增加随机角度
        "need_crop": True,      # 是否要增加裁剪
        "need_flip": True,      # 是否要增加水平随机翻转
        "hue_prob": 0.5,
        "hue_delta": 18,
        "contrast_prob": 0.5,
        "contrast_delta": 0.5,
        "saturation_prob": 0.5,
        "saturation_delta": 0.5,
        "brightness_prob": 0.5,
        "brightness_delta": 0.125
    },
    "early_stop": {
        "sample_frequency": 50,
        "successive_limit": 3,
        "good_acc1": 0.92
    },
    "rsm_strategy": {
        "learning_rate": 0.002,
        "lr_epochs": [20, 40, 60, 80, 100],
        "lr_decay": [1, 0.5, 0.25, 0.1, 0.01, 0.002]
    },
    "momentum_strategy": {
        "learning_rate": 0.002,
        "lr_epochs": [20, 40, 60, 80, 100],
        "lr_decay": [1, 0.5, 0.25, 0.1, 0.01, 0.002]
    },
    "sgd_strategy": {
        "learning_rate": 0.002,
        "lr_epochs": [20, 40, 60, 80, 100],
        "lr_decay": [1, 0.5, 0.25, 0.1, 0.01, 0.002]
    },
    "adam_strategy": {
        "learning_rate": 0.002
    }
}


def init_train_parameters():
    """
    初始化训练参数，主要是初始化图片数量，类别数
    :return:
    """
    train_file_list = os.path.join("/home/aistudio", train_parameters['train_file_list'])
    # label_list = os.path.join(train_parameters['data_dir'], train_parameters['label_file'])
    # index = 0
    # with codecs.open(label_list, encoding='utf-8') as flist:
    #     lines = [line.strip() for line in flist]
    #     for line in lines:
    #         parts = line.strip().split()
    #         train_parameters['label_dict'][parts[1]] = int(parts[0])
    #         index += 1
    #     train_parameters['class_dim'] = index
    with codecs.open(train_file_list, encoding='utf-8') as flist:
        lines = [line.strip() for line in flist]
        train_parameters['image_count'] = len(lines)

```

### 日志输出配置


```python
# 设置一个全局的日志变量
global logger 
logger = logging.getLogger() 
logger.setLevel(logging.INFO) 
log_path = os.path.join(os.getcwd(), 'logs') #当前目录下创建log文件夹
if not os.path.exists(log_path): 
    os.makedirs(log_path) 
log_name = os.path.join(log_path, 'train.log') #日志文件的名字
# sh = logging.StreamHandler() 
fh = logging.FileHandler(log_name, mode='w') 
fh.setLevel(logging.DEBUG) 
formatter = logging.Formatter("%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s") 
fh.setFormatter(formatter) 
# sh.setFormatter(formatter) 
# logger.addHandler(sh) 
logger.addHandler(fh) 
# 记录此次运行的超参,方便日后做记录进行比对
logger.info(train_parameters)

# tips:小伙伴们可以在这里初始化日志的时候加上日期,更为方便.
```




为了增加训练集的数据量，提高模型的泛化能力，对训练集进行数据增强处理

应用数据增强技术，对已有图片做缩放、随机旋转、随机裁剪、对比度调整、色调调整以及饱和度调整，数据增强后，大幅提升了训练样本数量。

为了之后的使用方便,进行了封装

对输入的图片进行归一化，保证输入的信息类型一致。



```python
# tips:因为我已经封装成为文件了,你可以仔细读读reader.py文件的

##获取猫儿数据
train_reader = paddle.batch(reader.train(), batch_size=32)
test_reader = paddle.batch(reader.val(), batch_size=32)
logger.info("成功加载数据") 




# tips:解除注释,可以查看一下获得的数据是什么样子的
# print(train_reader())
# sampledata=next(train_reader())
# print(sampledata)
```




### ResNet 模型网络结构


```python
class ResNet(object):
    """
    resnet的网络结构类
    """

    def __init__(self, layers=101):
        """
        resnet的网络构造函数
        :param layers: 网络层数
        """
        self.layers = layers

    def name(self):
        """
        获取网络结构名字
        :return:
        """
        return 'resnet'

    def net(self, input, class_dim=34):
        """
        构建网络结构
        :param input: 输入图片
        :param class_dim: 分类类别
        :return:
        """
        layers = self.layers
        supported_layers = [50, 101, 152]
        assert layers in supported_layers, \
            "supported layers are {} but input layer is {}".format(supported_layers, layers)

        if layers == 50:
            depth = [3, 4, 6, 3]
        elif layers == 101:
            depth = [3, 4, 23, 3]
        elif layers == 152:
            depth = [3, 8, 36, 3]
        num_filters = [64, 128, 256, 512]

        conv = self.conv_bn_layer(
            input=input,
            num_filters=64,
            filter_size=7,
            stride=2,
            act='relu',
            name="conv1")
        conv = fluid.layers.pool2d(
            input=conv,
            pool_size=3,
            pool_stride=2,
            pool_padding=1,
            pool_type='max')

        for block in range(len(depth)):
            for i in range(depth[block]):
                if layers in [101, 152] and block == 2:
                    if i == 0:
                        conv_name = "res" + str(block + 2) + "a"
                    else:
                        conv_name = "res" + str(block + 2) + "b" + str(i)
                else:
                    conv_name = "res" + str(block + 2) + chr(97 + i)
                conv = self.bottleneck_block(
                    input=conv,
                    num_filters=num_filters[block],
                    stride=2 if i == 0 and block != 0 else 1,
                    name=conv_name)
                # print("conv:", conv.shape)
                # print("conv:", type(conv))

        pool = fluid.layers.pool2d(input=conv, pool_size=7, pool_type='avg', global_pooling=True)
        stdv = 1.0 / math.sqrt(pool.shape[1] * 1.0)
        out = fluid.layers.fc(input=pool,
                              size=class_dim,
                              act='softmax',
                              param_attr=fluid.param_attr.ParamAttr(initializer=Uniform(-stdv, stdv)))
        return out

    def conv_bn_layer(self,
                      input,
                      num_filters,
                      filter_size,
                      stride=1,
                      groups=1,
                      act=None,
                      name=None):
        """
        便捷型卷积结构，包含了batch_normal处理
        :param input: 输入图片
        :param num_filters: 卷积核个数
        :param filter_size: 卷积核大小
        :param stride: 平移
        :param groups: 分组
        :param act: 激活函数
        :param name: 卷积层名字
        :return:
        """
        conv = fluid.layers.conv2d(
            input=input,
            num_filters=num_filters,
            filter_size=filter_size,
            stride=stride,
            padding=(filter_size - 1) // 2,
            groups=groups,
            act=None,
            param_attr=ParamAttr(name=name + "_weights"),
            bias_attr=False,
            name=name + '.conv2d.output.1')
        if name == "conv1":
            bn_name = "bn_" + name
        else:
            bn_name = "bn" + name[3:]
        return fluid.layers.batch_norm(
            input=conv,
            act=act,
            name=bn_name + '.output.1',
            param_attr=ParamAttr(name=bn_name + '_scale'),
            bias_attr=ParamAttr(bn_name + '_offset'),
            moving_mean_name=bn_name + '_mean',
            moving_variance_name=bn_name + '_variance', )

    def shortcut(self, input, ch_out, stride, name):
        """
        转换结构，转换输入和输出一致，方便最后的短链接结构
        :param input:
        :param ch_out:
        :param stride:
        :param name:
        :return:
        """
        ch_in = input.shape[1]
        if ch_in != ch_out or stride != 1:
            return self.conv_bn_layer(input, ch_out, 1, stride, name=name)
        else:
            return input

    def bottleneck_block(self, input, num_filters, stride, name):
        """
        resnet的短路链接结构中的一种，采用压缩方式先降维，卷积后再升维
        利用转换结构将输入变成瓶颈卷积一样的尺寸，最后将两者按照位相加，完成短路链接
        :param input:
        :param num_filters:
        :param stride:
        :param name:
        :return:
        """
        conv0 = self.conv_bn_layer(
            input=input,
            num_filters=num_filters,
            filter_size=1,
            act='relu',
            name=name + "_branch2a")
        conv1 = self.conv_bn_layer(
            input=conv0,
            num_filters=num_filters,
            filter_size=3,
            stride=stride,
            act='relu',
            name=name + "_branch2b")
        conv2 = self.conv_bn_layer(
            input=conv1,
            num_filters=num_filters * 4,
            filter_size=1,
            act=None,
            name=name + "_branch2c")

        short = self.shortcut(
            input, num_filters * 4, stride, name=name + "_branch1")

        return fluid.layers.elementwise_add(
            x=short, y=conv2, act='relu', name=name + ".add.output.5")

```

### 不同类型优化器定义


```python
def optimizer_momentum_setting():
    """
    阶梯型的学习率适合比较大规模的训练数据
    """
    learning_strategy = train_parameters['momentum_strategy']
    batch_size = train_parameters["train_batch_size"]
    iters = train_parameters["image_count"] // batch_size
    lr = learning_strategy['learning_rate']

    boundaries = [i * iters for i in learning_strategy["lr_epochs"]]
    values = [i * lr for i in learning_strategy["lr_decay"]]
    learning_rate = fluid.layers.piecewise_decay(boundaries, values)
    optimizer = fluid.optimizer.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)
    return optimizer


def optimizer_rms_setting():
    """
    阶梯型的学习率适合比较大规模的训练数据
    """
    batch_size = train_parameters["train_batch_size"]
    iters = train_parameters["image_count"] // batch_size
    learning_strategy = train_parameters['rsm_strategy']
    lr = learning_strategy['learning_rate']

    boundaries = [i * iters for i in learning_strategy["lr_epochs"]]
    values = [i * lr for i in learning_strategy["lr_decay"]]

    optimizer = fluid.optimizer.RMSProp(
        learning_rate=fluid.layers.piecewise_decay(boundaries, values))

    return optimizer


def optimizer_sgd_setting():
    """
    loss下降相对较慢，但是最终效果不错，阶梯型的学习率适合比较大规模的训练数据
    """
    learning_strategy = train_parameters['sgd_strategy']
    batch_size = train_parameters["train_batch_size"]
    iters = train_parameters["image_count"] // batch_size
    lr = learning_strategy['learning_rate']

    boundaries = [i * iters for i in learning_strategy["lr_epochs"]]
    values = [i * lr for i in learning_strategy["lr_decay"]]
    learning_rate = fluid.layers.piecewise_decay(boundaries, values)
    optimizer = fluid.optimizer.SGD(learning_rate=learning_rate)
    return optimizer


def optimizer_adam_setting():
    """
    能够比较快速的降低 loss，但是相对后期乏力
    """
    learning_strategy = train_parameters['adam_strategy']
    learning_rate = learning_strategy['learning_rate']
    optimizer = fluid.optimizer.Adam(learning_rate=learning_rate)
    return optimizer
```

### 模型数据加载
可以加载预训练模型，也可以加载上一次训练的参数


```python
def load_params(exe, program):
    if train_parameters['continue_train'] and os.path.exists(train_parameters['save_persistable_dir']):
        logger.info('load params from retrain model')
        fluid.io.load_persistables(executor=exe,
                                   dirname=train_parameters['save_persistable_dir'],
                                   main_program=program)
    elif train_parameters['pretrained'] and os.path.exists(train_parameters['pretrained_dir']):
        logger.info('load params from pretrained model')
        def if_exist(var):
            return os.path.exists(os.path.join(train_parameters['pretrained_dir'], var.name))

        fluid.io.load_vars(exe, train_parameters['pretrained_dir'], main_program=program,
                           predicate=if_exist)
```

### 定义绘制训练过程的损失值和准确率变化趋势的方法draw_train_process**


```python

def draw_train_process(title,iters,costs,accs,label_cost,lable_acc):
    # print(11111)
    plt.title(title, fontsize=24)
    plt.xlabel("iter", fontsize=20)
    plt.ylabel("cost/acc", fontsize=20)
    plt.plot(iters, costs,color='red',label=label_cost) 
    plt.plot(iters, accs,color='green',label=lable_acc) 
    plt.legend()
    plt.grid()
    plt.show()
```

### 训练循环主体，开始愉快的炼丹吧~


```python
def train():
    all_train_iter=0
    all_train_iters=[]
    all_train_costs=[]
    all_train_accs=[]
    all_val_costs=[]
    all_val_accs=[]
    train_prog = fluid.Program()
    train_startup = fluid.Program()
    logger.info("create prog success")
    logger.info("train config: %s", str(train_parameters))
    logger.info("build input custom reader and data feeder")
    # file_list = os.path.join(train_parameters['data_dir'], "train.txt")
    # mode = train_parameters['mode']
    # batch_reader = paddle.batch(custom_image_reader(file_list, train_parameters['data_dir'], mode),
    #                             batch_size=train_parameters['train_batch_size'],
    #                             drop_last=True)
    place = fluid.CUDAPlace(0) if train_parameters['use_gpu'] else fluid.CPUPlace()
    # 定义输入数据的占位符
    img = fluid.layers.data(name='img', shape=train_parameters['input_size'], dtype='float32')
    label = fluid.layers.data(name='label', shape=[1], dtype='int64')
    feeder = fluid.DataFeeder(feed_list=[img, label], place=place)

    # 选取不同的网络
    logger.info("build newwork")
    model = ResNet(layers=101)
    out = model.net(input=img, class_dim=train_parameters['class_dim'])

    cost = fluid.layers.cross_entropy(out, label)
    avg_cost = fluid.layers.mean(x=cost)
    acc = fluid.layers.accuracy(input=out, label=label, k=1)

    ##获取训练和测试程序
    test_program = fluid.default_main_program().clone(for_test=True)

    # 选取不同的优化器
    # optimizer = optimizer_rms_setting()
    # optimizer = optimizer_momentum_setting()
    optimizer = optimizer_sgd_setting()
    # optimizer = optimizer_adam_setting()
    optimizer.minimize(avg_cost)
    exe = fluid.Executor(place)

    main_program = fluid.default_main_program()
    exe.run(fluid.default_startup_program())
    fetch_list = [avg_cost, acc]
    
    load_params(exe, main_program)

    # 训练循环主体
    stop_strategy = train_parameters['early_stop']
    successive_limit = stop_strategy['successive_limit']
    sample_freq = stop_strategy['sample_frequency']
    good_acc1 = stop_strategy['good_acc1']
    successive_count = 0
    stop_train = False
    total_batch_count = 0
    last_acc=train_parameters["last_acc"]
    # 保存模型的位置
    save_pretrain_model_path='./freeze-model'
    # 初始的准确率
    now_acc=0
    # train_cost = 0
    # train_acc = 0
    resultFile_train=open('result_train.txt','a')
    resultFile_val=open('result_val.txt','a')
    for pass_id in range(train_parameters["num_epochs"]):
        logger.info("current pass: %d, start read image", pass_id)
        batch_id = 0
        # print(1111)
        for batch_id, data in enumerate(train_reader()):
            t1 = time.time()
            train_cost, train_acc = exe.run(main_program,
                                          feed=feeder.feed(data),
                                          fetch_list=fetch_list)
            # print(2222)
            if batch_id%50==0:
                print('Pass:%d, Batch:%d, Cost:%0.5f, Accuracy:%0.5f' %
                  (pass_id, batch_id, train_cost[0], train_acc[0]))
            # if batch_id%150==0:
            #     all_train_iter=all_train_iter+16
            #     all_train_iters.append(all_train_iter)
            #     all_train_costs.append(train_cost[0])
            #     all_train_accs.append(train_acc[0])
                # print("此时的batch_id={0}".format(batch_id))
        # print(3333)
        all_train_iters.append(all_train_iter)
        all_train_costs.append(train_cost[0])
        all_train_accs.append(train_acc[0])
        resultFile_train.write(str(all_train_iter) + "\t" + str(train_cost[0]) + "\t" + str(train_acc[0]))
        resultFile_train.write("\n")
        ##测试
        test_accs=[]
        test_costs=[]
        for batch_id,data in enumerate(test_reader()):
            test_cost,test_acc=exe.run(program=test_program,feed=feeder.feed(data), fetch_list=[avg_cost,acc])
            test_accs.append(test_acc[0])
            test_costs.append(test_cost[0])
        all_val_costs.append(test_cost[0])
        all_val_accs.append(test_acc[0])
        resultFile_val.write(str(all_train_iter) + "\t" + str(test_cost[0]) + "\t" + str(test_acc[0]))
        resultFile_val.write("\n")
        all_train_iter=all_train_iter+1
        test_cost = (sum(test_costs) / len(test_costs))
        test_acc = (sum(test_accs) / len(test_accs))
        logger.info('Test:%d, Cost:%0.5f, Accuracy:%0.5f' % (pass_id, test_cost, test_acc))
        now_acc=test_acc
        if now_acc>last_acc and now_acc!=1:
            last_acc=now_acc
            logger.info("临时保存第{0}批次的训练结果，准确率为{1}".format(pass_id, now_acc)) 
            ##删除旧的模型文件
            shutil.rmtree(save_pretrain_model_path,ignore_errors=True)
            ##创建保存模型文件记录
            os.makedirs(save_pretrain_model_path)
            ##保存参数模型
            fluid.io.save_persistables(dirname=train_parameters['save_persistable_dir'],
                                                   main_program=main_program,
                                                   executor=exe)
            fluid.io.save_inference_model(dirname=train_parameters['save_freeze_dir'],
                                                      feeded_var_names=['img'],
                                                      target_vars=[out],
                                                      main_program=main_program,
                                                      executor=exe)
        if stop_train:
            break
    logger.info("training till last epcho, end training")
    draw_train_process("training",all_train_iters,all_train_costs,all_train_accs,"trainning cost","trainning acc")
    draw_train_process("valing",all_train_iters,all_val_costs,all_val_accs,"valing cost","valing acc")


if __name__ == '__main__':
    # init_log_config()
    init_train_parameters()
    train()
```


### 加载保存的模型进行验证


```python
from __future__ import absolute_import    
from __future__ import division    
from __future__ import print_function    
    
import os    
import numpy as np    
import random    
import time    
import codecs    
import sys    
import functools    
import math    
import paddle    
import paddle.fluid as fluid    
from paddle.fluid import core    
from paddle.fluid.param_attr import ParamAttr    
from PIL import Image, ImageEnhance   
 

DATA_DIM = 224
target_size = [3, 224, 224]    
mean_rgb = [127.5, 127.5, 127.5]    
data_dir = "data/data87406"    
eval_file = "test_list.txt"    
use_gpu = True    
place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()    
exe = fluid.Executor(place)    
save_freeze_dir = "./freeze-model"    
[inference_program, feed_target_names, fetch_targets] = fluid.io.load_inference_model(dirname=save_freeze_dir, executor=exe)    
# print(fetch_targets)    
    

def crop_image(img, target_size, center):
    width, height = img.size
    size = target_size
    if center == True:
        w_start = (width - size) / 2
        h_start = (height - size) / 2
    else:
        w_start = np.random.randint(0, width - size + 1)
        h_start = np.random.randint(0, height - size + 1)
    w_end = w_start + size
    h_end = h_start + size
    img = img.crop((w_start, h_start, w_end, h_end))
    return img
    
def resize_short(img, target_size):
    percent = float(target_size) / min(img.size[0], img.size[1])
    resized_width = int(round(img.size[0] * percent))
    resized_height = int(round(img.size[1] * percent))
    img = img.resize((resized_width, resized_height), Image.LANCZOS)
    return img

img_mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))
img_std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))
    
def read_image(img_path):    
    img = Image.open(img_path)
    img = resize_short(img, target_size=256)
    img = crop_image(img, target_size=DATA_DIM, center=True)

    if img.mode != 'RGB':
        img = img.convert('RGB')

    img = np.array(img).astype(np.float32).transpose((2, 0, 1)) / 255
    img -= img_mean
    img /= img_std

    img = np.expand_dims(img, axis=0)
    return img    
    
    
def infer(image_path):    
    tensor_img = read_image(image_path)    
    label = exe.run(inference_program, feed={feed_target_names[0]: tensor_img}, fetch_list=fetch_targets)    
    # print("label:", label)
    # print("np.argmax(label) :", np.argmax(label) )
    return np.argmax(label)   
 

def convert_list(my_list):
    my_list = list(my_list)
    my_list = map(lambda x:str(x), my_list)
    # print('_'.join(my_list))
    return '_'.join(my_list)
    
# 字符串转数字
def str2int(s):
    try:
        return int(s)
    except:
        if('-'==s[0]):
            return 0 - str2int(s[1:])
        elif s[0] in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:
            num = 0
            for i in range(len(s) ):
                if s[i] in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:
                    num = num * 10 + int(s[i])
                else:
                    return num
        else:
            return 0

def eval_all():     
    eval_file_path = os.path.join(data_dir, eval_file)     
    total_count = 0     
    right_count = 0     
    right_quantity = []
    all_quantity = []
    for i in range(2):
        right_quantity.append(0)
        all_quantity.append(0)
    with codecs.open(eval_file_path, encoding='utf-8') as flist:      
        lines = [line.strip() for line in flist]     
        t1 = time.time()     
        for line in lines:     
            total_count += 1     
            parts = line.strip().split()     
            result = infer(parts[0])     
            # print("infer result:{0} answer:{1}".format(result, parts[1]))     
            if str(result) == parts[1]:     
                right_quantity[str2int(result)] = right_quantity[str2int(result)]+1
                right_count += 1     
            all_quantity[str2int(parts[1])] = all_quantity[str2int(parts[1])]+1
        period = time.time() - t1     
        for i in range(2):
            all_quantity[i] = "%.2f%%" % (right_quantity[i] / all_quantity[i] * 100)
        print("测试集总数:{0}  测试花费时间:{1}  预测总体准确率:{2}".format(total_count, "%2.2f 秒" % period, "%.2f%%" % (right_count / total_count * 100)))     
        print("每个类别测试对应准确率为:{0}".format(all_quantity))    
    
    
if __name__ == '__main__':    
    eval_all()  
```